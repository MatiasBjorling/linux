Hybrid storage cache

The idea is described in the btrfs wiki. We kindly ask for comments
regarding implementing this hybrid storage cache.

1. Introduction

The emerge of fast Solid State Storage (SSD) change data how is located for fast 
accesses. Their high throughput and low latency characteristics make them a good
choice for applications that traditionally require many traditional hard-drives.
However, SSDs is more expensive per GB and as such, traditional hard-drives are
still efficient to store bulk amount of data. Often, the working set of a 
filesystem is smaller than the full capacity of a drive. We can exploit this in 
an intelligent matter. The working sets are stored as a caching layer on SSDs. 
They SSDs then provide fast random accesses, while larger bulk operations are 
executed on the bulk storage.

Resent development in Linux SSD caches takes a block IO approach to solve
caching. The caches are good for use-cases where data is stable and evict data
using based on LRU, Temparature, etc. It ignores the benefits of append 
kernel is good enough. However, the caches lack internal file system knowledge,
that isn't easily available to block IO caches. The internal 

2. Design
The design space for the cache is divided into read and writes. For both reads
and writes we can divide the cache into caching metadata (trees) accesses or the
user data. Writes are further divided into either being write-back or write-through.



2.1 Overview

Any device attached to the storage pool should allow to be used as a cache. We
propose that the cache is stored in chunks (as cache chunks). Each allocated
cache chunk is then available to one or more subvolumes.

Why not not use a block cache ?

2.2 Caching hierarchy

The extra layer created the following access pattern: memory -> SSD/Disk (example) -> Disk

* Host memory caches lookup paths, transactions, free space infomation, etc.
* SSD/disk caches frequently used data.
* Traditional disks stores the largest amount of data.

2.2 Cache opportunities:

- Hotness tracking for random reads

  Define threshold for when to cache reads. Back of envelope calculation
  tells us to cache when IO size is below 1.5MB. This assumes 100 IO/s and
  a read speed of 150MB/s from the traditional drives. This should be
  tunable.

  If data is updated, we should follow the new data and evict the "old" data and
  pre-cache the new data.

  Implementation details:
    - Use the hot track patches for VFS to track when an inode is hot and then
      cache the reads.
    - Track CoW actions and pre-warm cache.

- Write-back cache 
  
  * Tree updates

    Updates to trees are batched and flushed every 30 seconds. Flush the updates to
    cache layer first, and then flush them later to bulk storage.

    When updates are flushed to bulk storage, we reorder IOs to be as sequential
    as possible. This optimization allows us to have higher throughput at
    the cost of sorting writes at flush time.

    The technique requires that we track tree updates between disk cache and
    disk. As our trees are append only, we can track the current generation and
    apply the difference at timed intervals or at mount/unmount times.

    Implementation details: 
      -
  * Data updates

    

 - Write-through cache for user data

   If the cache isn't "safe", i.e. no duplicate copies. The cache can still be 
   used using write-through and cache subsequent reads.

   This is similar to a pure disk block-based cache approach. 


2.3 Other

 - Warm cache at mount time

   Reread the SSD cache on mount to enjoy a semi-warm cache of the bulk storage.

The following list of items have to be addressed for the soluton:

* Cache lookup (Stored in tree, hash table, in ssd hash table ;))
* Cache type (write through, write back, hot tracking, etc.)
* Data structure for lookup cache
* Allow for prioritized storage (PCM>SSD>HDD)
* Eviction strategy. LRU, LFU, FIFO, Temparature-based (VFS hot track)

2.1 Implementation

* Add new cache flag type for cache block group. (referenced in 
  btrfs_block_group_item)
*

2.1 Transaction cache

2.2 Data cache

2.2.1 Read cache

2.2.2 Write cache

* Write-through 
* Write-back
* Read-ahead opportunities
* 
