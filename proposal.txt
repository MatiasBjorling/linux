Hybrid storage cache

The hybrid storage cache is shortly discussed in the btrfs wiki. We (Jesper and I) kindly ask for
comments regarding implemention of this hybrid storage cache. We have written a
short proposal with introduction/motivation and a bird's eye view of a possible
approach. There is currently no available patches, as we like as much input before
we begin to implement a solution. We are aware that it is no small task and
expect to spend considerable amount of time on it.

1. Introduction

The emerge of fast Solid State Drives (SSD) change how data is stored for fast 
accesses. Their high throughput and low latency characteristics make them a good
choice for applications that traditionally require many traditional hard-drives.
However, SSDs is more expensive per GB and as such, traditional hard-drives are
still efficient to store bulk amount of data. Often, the working set of a 
filesystem is smaller than the full capacity of a drive. We can exploit this in 
an intelligent matter. The working set is stored as a cache using SSDs. They then 
provide fast random accesses, while larger bulk operations are kept on
bulk storage.

Recent development in Linux SSD caches uses a block IO approach to solve
caching. The approach assumes that data is stable on disk and evicts data based on 
LRU, Temparature, etc. This is great for read only schenarios and in-place
writes. However, btrfs uses a copy on write approach, that reduces the benefits of 
block IO caching. The caches are unable to track updates (without extensive hints 
forth and back between the cache layer) and separate data and meta data updates.

The internal file-system information available within btrfs allows separation of
these types of updates and enables fine-grained control of a to-be implemented
cache system. 

2. Design

The design space for the cache is divided into read and writes. For both read
and write cache, we can divide it into caching metadata (trees) accesses or
user data. Writes are further divided into either being write-back or
write-through. 

2.1 Overview

Any device attached to the storage pool should allow to be used as a cache. We
propose that the cache is stored in chunks (as cache chunks). Each allocated
cache chunk is then available to one or more subvolumes.

2.2 Caching hierarchy

The extra layer create the following access pattern: memory -> SSD/Disk (example) -> Disk

* Host memory caches lookup paths, transactions, free space infomation, etc.
* SSD/disk cache frequently used data or writes for data that cannot be in host
  memory.
* Traditional hard-drives store the largest amount of data and have a complete
  copy of all data.

2.2 Cache opportunities:

- Hotness tracking for random reads

  Define threshold for when to cache reads. Back of envelope calculation
  tells us to cache when IO size is below 1.5MB. This assumes 100 IO/s and
  a read speed of 150MB/s from the traditional drives. This should be
  tunable.

  If data is updated, we should follow the new data and evict the "old" data and
  pre-cache the new data.

  Implementation details:
    - Use the hot track patches for VFS to track when an inode is hot and then
      cache the reads.
    - Track CoW actions and pre-warm cache.

- Write-back cache 

  * Tree updates

    Updates to trees are batched and flushed every 30 seconds. Flush the updates to
    cache layer first, and then flush them later to bulk storage.

    When updates are flushed to bulk storage, we reorder IOs to be as sequential
    as possible. This optimization allows us to have higher throughput at
    the cost of sorting writes at flush time.

    The technique requires that we track tree updates between disk cache and
    disk. As our trees are append only, we can track the current generation and
    apply the difference at timed intervals or at mount/unmount times.

    Implementation details: 
      -
  * Data updates

 - Write-through cache for user data

   If the cache isn't "safe", i.e. no duplicate copies. The cache can still be 
   used using write-through and cache subsequent reads.

   This is similar to a pure disk block-based cache approach. 


2.3 Other

 - Warm cache at mount time

   Reread the SSD cache on mount to enjoy a semi-warm cache of the bulk storage.

   This can be archived by storing information about the cache and reconstruct
   the cache tree 

The following list of items have to be addressed for the soluton:

* Cache lookup (Stored in tree, hash table, in ssd hash table ;))
* Cache type (write through, write back, hot tracking, etc.)
* Data structure for lookup cache
* Allow for prioritized storage (PCM>SSD>HDD)
* Eviction strategy. LRU, LFU, FIFO, Temparature-based (VFS hot track)

2.1 Implementation

* Add new cache flag type for cache block group. (referenced in 
  btrfs_block_group_item)
